/* Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License. */
#ifdef PADDLE_WITH_XPU_KP

#include <xpu/runtime.h>  // NOLINT
#include <algorithm>
#include <ctime>
#include <memory>
#include <numeric>

#include "paddle/fluid/memory/memcpy.h"
#include "paddle/fluid/platform/device/xpu/enforce_xpu.h"
#include "paddle/fluid/platform/device_context.h"

#include "xpu/kernel/xtdk.h"  // NOLINT
#include "xpu/kernel/xtdk_math.h"            // NOLINT
#include "xpu/kernel/xtdk_simd.h"

#include "paddle/fluid/operators/batch_fc_op.h"
#include "paddle/phi/kernels/funcs/math_function.h"

#include "paddle/fluid/operators/xpu_api_wrapper.h"

namespace paddle {
namespace operators {
using framework::Tensor;

static __device__ void primitive_add(const float* x, const float* y, float* z, int len) {
    float32x16_t vx0;
    float32x16_t vy0;
    float32x16_t vx1;
    float32x16_t vy1;
    int len_rounddown32 = rounddown32(len);
    int remain = len - len_rounddown32;
    for (int i = 0; i < len_rounddown32; i += 32) {
        vx0 = vload_lm_float32x16(x + i);
        vx1 = vload_lm_float32x16(x + i + 16);
        vy0 = vload_lm_float32x16(y + i);
        vy1 = vload_lm_float32x16(y + i + 16);
        vy0 = vvadd_float32x16(vx0, vy0);
        vy1 = vvadd_float32x16(vx1, vy1);
        vstore_lm_float32x16(z + i, vy0);
        vstore_lm_float32x16(z + i + 16, vy1);
    }
    for (int i = 0; i < remain; i++) {
      *(z + len_rounddown32 + i) = *(y + len_rounddown32 + i) + *(x + len_rounddown32 + i);
    }
    mfence_lm();
}

static __device__ inline void memset_lm_float(float* dst_ptr, int size) {
    for (int i = 0; i < size; i += 16) {
        vstore_lm_float32x16_mz(dst_ptr + i, 0, 0);
    }
    mfence_lm();
}

template <typename T>
__global__ void add_bias_kernel(
    T* data, int slot_pairs_num, int ins_num, int out_dim, const T* bias) {
  int cid = core_id();
  int ncores = core_num();
  if (cid >= ncores) {
    return;
  }
  int thread_id = cluster_id() * ncores + cid;
  int total_thread = cluster_num() * ncores;

  const int buf_size = 512;
  int max_seq_len = buf_size / out_dim;
  int bias_buff_len = roundup16(out_dim);

  __simd__ T local_data_buf[buf_size];
  __simd__ T local_bias_buf[bias_buff_len];

  __simd__ T out_buf[buf_size];
  memset_lm_float(out_buf, buf_size);

  for (int64_t i = thread_id * max_seq_len; i < ins_num; i += total_thread * max_seq_len) {
    int len = min(max_seq_len, ins_num - i);
    for (int slot = 0; slot < slot_pairs_num; slot++) {
      mfence();
      GM2LM(bias + slot * out_dim, local_bias_buf,  out_dim * sizeof(T));

      GM2LM(data + slot * ins_num * out_dim + i * out_dim, local_data_buf, len * out_dim * sizeof(T));
      for (int j = 0; j < len; j++) {
        primitive_add(local_data_buf + j * out_dim, local_bias_buf, out_buf + j * out_dim, out_dim);
      }
      // mfence();
      LM2GM_ASYNC(out_buf, data + slot * ins_num * out_dim + i * out_dim, len * out_dim * sizeof(T));
    }
  }
}

template <typename T>
void add_bias(xpu::Context* xpu_ctx,
              T* data,
              int slot_pairs_num,
              int ins_num,
              int out_dim,
              const T* bias) {
  xpu::ctx_guard RAII_GUARD(xpu_ctx);
  auto stream = xpu_ctx->xpu_stream;              
  add_bias_kernel<<<xpu_ctx->ncluster(), 64, stream>>>(data, slot_pairs_num, ins_num, out_dim, bias);
}

template <typename T>
__global__ void add_bias_grad_kernel(const T* dout_data,
                                     int slot_pairs_num,
                                     int ins_num,
                                     int out_dim,
                                     T* db_data) {
  int cid = core_id();
  int ncores = core_num();
  if (cid >= ncores) {
    return;
  }
  int thread_id = cluster_id() * ncores + cid;
  int total_thread = cluster_num() * ncores;

  int buf_size = out_dim + 16;
  __simd__ T local_bias_buf[buf_size];
  __simd__ T tmp_sum_buf[buf_size];

  __local__ T local_data_buf[1];

  // memset_lm_float(local_bias_buf, buf_size);
  memset_lm_float(tmp_sum_buf, buf_size);

  __local__ T tmp_sum = static_cast<T>(0);
  for (int i = thread_id; i < ins_num; i += total_thread) { 
    for (int slot = 0; slot < slot_pairs_num; slot++) {
      mfence();
      GM2LM(db_data + slot * out_dim, local_bias_buf, out_dim * sizeof(T));

      for (int index = 0; index < out_dim; index++) {
        int select_indx = ((slot + 1) * i + 1) * index;
        GM2LM(dout_data + select_indx, local_data_buf, sizeof(T));
        // mfence();
        tmp_sum_buf[index] += local_data_buf[0];
      }

      // mfence();
      primitive_add(tmp_sum_buf, local_bias_buf, local_bias_buf, out_dim);

      // mfence();
      LM2GM_ASYNC(local_bias_buf, db_data + slot * out_dim, out_dim * sizeof(T));
      // mfence();
    }
  }
}

template <typename T>
void add_bias_grad(xpu::Context* xpu_ctx,
                   const T* dout_data,
                   int slot_pairs_num,
                   int ins_num,
                   int out_dim,
                   T* db_data) {
  xpu::ctx_guard RAII_GUARD(xpu_ctx);
  auto stream = xpu_ctx->xpu_stream;   
  add_bias_grad_kernel<<<xpu_ctx->ncluster(), 64, stream>>>(
      dout_data, slot_pairs_num, ins_num, out_dim, db_data);
}

template <typename T>
class BatchFCXPUKernel : public framework::OpKernel<T> {
  using XPUType = typename XPUTypeTrait<T>::Type;

 public:
  void Compute(const framework::ExecutionContext& ctx) const override {
    int batchcount = ctx.Attr<int>("batchcount");
    auto transpose_weight = ctx.Attr<bool>("transpose_weight");
    if (transpose_weight) {
      // TODO
      PADDLE_ENFORCE_EQ(
        transpose_weight,
        true,
        platform::errors::Unimplemented("BatchFC not support transpose_weight now."));
      return;
    }
    if (batchcount > 0) {
      // TODO
      PADDLE_ENFORCE_EQ(
        (batchcount > 0),
        true,
        platform::errors::Unimplemented("BatchFC not support transpose_weight now."));
    } else {
      // X.dim = slot_pairs_num * ins_num * in_dim
      // W.dim = slot_pairs_num * in_dim * out_dim
      // b.dim = slot_pairs_num * out_dim
      // output.dim = slot_pairs_num * ins_num * out_dim
      auto* input = ctx.Input<framework::LoDTensor>("Input");
      auto* w = ctx.Input<Tensor>("W");
      auto* bias = ctx.Input<Tensor>("Bias");
      auto* output = ctx.Output<framework::LoDTensor>("Out");
      auto input_dims = input->dims();
      auto w_dims = w->dims();
      auto slot_pairs_num = input_dims[0];
      auto ins_num = input_dims[1];
      auto out_dim = w_dims[2];
  
      // get data ptr
      const XPUType* x_ptr = reinterpret_cast<const XPUType*>(input->data<T>());
      const XPUType* y_ptr = reinterpret_cast<const XPUType*>(w->data<T>());
      const XPUType* bias_data = reinterpret_cast<const XPUType*>(bias->data<T>());
  
      output->Resize({slot_pairs_num, ins_num, out_dim});
      XPUType* out_ptr = reinterpret_cast<XPUType*>(output->mutable_data<T>(ctx.GetPlace()));

      // initialize
      auto xpu_context =
        ctx.template device_context<paddle::platform::XPUDeviceContext>().x_context();
  
      bool trans_x = false;
      bool trans_y = false;
  
      T alpha = 1;
     
      XpuFcInfo fc_info;
      GetFCInfo(input_dims, w_dims, trans_x, trans_y, &fc_info);
      MatMulXPUFunction<XPUType>(xpu_context, x_ptr, y_ptr, out_ptr, fc_info, alpha);
  
      // add bias
      add_bias<T>(xpu_context,
                  out_ptr,
                  slot_pairs_num,
                  ins_num,
                  out_dim,
                  bias_data);
    }
  }
};

template <typename T>
class BatchFCGradOpXPUKernel : public framework::OpKernel<T> {
  using XPUType = typename XPUTypeTrait<T>::Type;

 public:
  void Compute(const framework::ExecutionContext& ctx) const override {
    int batchcount = ctx.Attr<int>("batchcount");
    if (batchcount > 0) {
      // TODO
      PADDLE_ENFORCE_EQ(
        (batchcount > 0),
        true,
        platform::errors::Unimplemented("BatchFC not support transpose_weight now."));
    } else {
      auto* input = ctx.Input<Tensor>("Input");
      auto* w = ctx.Input<Tensor>("W");
      auto* dout = ctx.Input<Tensor>(framework::GradVarName("Out"));
  
      auto* dx = ctx.Output<Tensor>(framework::GradVarName("Input"));
      auto* dw = ctx.Output<Tensor>(framework::GradVarName("W"));
      auto* db = ctx.Output<Tensor>(framework::GradVarName("Bias"));
  
      auto input_dims = input->dims();
      auto w_dims = w->dims();
      auto slot_pairs_num = input_dims[0];
      auto ins_num = input_dims[1];
      auto out_dim = w_dims[2];

      const XPUType* dout_ptr = reinterpret_cast<const XPUType*>(dout->data<T>());
      const XPUType* x_ptr = reinterpret_cast<const XPUType*>(dx->mutable_data<T>(ctx.GetPlace()));
      const XPUType* y_ptr = reinterpret_cast<const XPUType*>(dw->mutable_data<T>(ctx.GetPlace()));
      XPUType* b_ptr = reinterpret_cast<XPUType*>(db->mutable_data<T>(ctx.GetPlace()));

      auto xpu_context =
        ctx.template device_context<paddle::platform::XPUDeviceContext>().x_context();
      xpu::ctx_guard RAII_GUARD(xpu_context);

      bool transpose_x = false;
      bool transpose_y = false;
      XpuFcInfo info_forward;
      GetFCInfo(input_dims, w_dims, transpose_x, transpose_y, &info_forward);

      const XPUType* a_1 = reinterpret_cast<const XPUType*>(NULL);
      const XPUType* b_1 = reinterpret_cast<const XPUType*>(NULL);
      const XPUType* a_2 = reinterpret_cast<const XPUType*>(NULL);
      const XPUType* b_2 = reinterpret_cast<const XPUType*>(NULL);
      XPUType* c_1 = (dx == NULL) ? reinterpret_cast<XPUType*>(NULL)
                                  : reinterpret_cast<XPUType*>(dx->data<T>());
      XPUType* c_2 = (dw == NULL) ? reinterpret_cast<XPUType*>(NULL)
                                  : reinterpret_cast<XPUType*>(dw->data<T>());

      // add bias grad
      add_bias_grad<T>(xpu_context,
                       dout_ptr,
                       slot_pairs_num,
                       ins_num,
                       out_dim,
                       b_ptr);

      // xpu_wait(xpu_context->xpu_stream);

      T alpha = 1;
      XpuFcInfo info_dx;
      XpuFcInfo info_dy;
      std::tuple<XpuFcInfo,
                XpuFcInfo,
                const XPUType*,
                const XPUType*,
                const XPUType*,
                const XPUType*>
          fc_info = MatmulGradFcInfo(xpu_context,
                                    &RAII_GUARD,
                                    info_forward,
                                    transpose_x,
                                    transpose_y,
                                    x_ptr,
                                    y_ptr,
                                    dout_ptr);
      std::tie(info_dx, info_dy, a_1, b_1, a_2, b_2) = fc_info;
  
      // dx = dout_data * y^T
      MatMulXPUFunction<XPUType>(xpu_context, a_1, b_1, c_1, info_dx, alpha);
      
      // dy = x^T * dout_data
      MatMulXPUFunction<XPUType>(xpu_context, a_2, b_2, c_2, info_dy, alpha);
    }
  }
};

}  // namespace operators
}  // namespace paddle

namespace ops = paddle::operators;
namespace plat = paddle::platform;

REGISTER_OP_KERNEL(batch_fc, KP, plat::XPUPlace,
                   ops::BatchFCXPUKernel<float>);     
REGISTER_OP_KERNEL(batch_fc_grad, KP, plat::XPUPlace,
                   ops::BatchFCGradOpXPUKernel<float>);    

REGISTER_OP_XPU_KERNEL(batch_fc,
                       ops::BatchFCXPUKernel<float>);      
REGISTER_OP_XPU_KERNEL(batch_fc_grad,
                       ops::BatchFCGradOpXPUKernel<float>);  
#endif
