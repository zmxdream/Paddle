/* Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

  http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License. */

#ifdef PADDLE_WITH_XPU_KP
#include <xpu/runtime.h>  // NOLINT
#include <algorithm>
#include <ctime>
#include <memory>
#include <numeric>

#include "paddle/fluid/memory/memcpy.h"
#include "paddle/fluid/platform/device/xpu/enforce_xpu.h"
#include "paddle/fluid/framework/fleet/box_wrapper_kernel.h"
#include "paddle/fluid/platform/device_context.h"
#include "xpu/kernel/cluster_header.h"  // NOLINT
// #include "xpu/kernel/debug.h"           // NOLINT
#include "xpu/kernel/math.h"            // NOLINT
#include "xpu/kernel/simd.h"
// The producer side.
#include <scalopus_tracing/tracing.h>
#include <scalopus_transport/transport_loopback.h>
// The catapult recorder side.
#include <scalopus_catapult/catapult_recorder.h>
#include <scalopus_general/endpoint_manager_poll.h>
#include <scalopus_general/general_provider.h>
#include <scalopus_tracing/native_trace_provider.h>

namespace paddle {
namespace framework {

struct EmbedxQuantOp {
  __device__ void copy(float* dest, const float* src,
                                       const int& idx,
                                       const float& scale) const {
    *dest = *(reinterpret_cast<const int16_t*>(src) + idx) * scale;
  }
};
struct EmbedxNormalOp {
  __device__ void copy(float* dest, const float* src,
                                       const int& idx,
                                       const float& /**scale*/) const {
    *dest = src[idx];
  }
};

template <typename T>
__device__ void set_byfloat(float* dest, const T& val) {
  (*reinterpret_cast<T*>(dest)) = val;
}

static inline __device__ void mfence_lm() {
    __asm__("mfence {lm}\n\t");
}

static inline __device__ void xpu_sync_all(int group_mask = -1) {
    __asm__("sync_local");
    __asm__("csr_set csr3, %0"::"r"(group_mask));
    __asm__("sync_group csr3");
}

__global__ void CopyKeysKernel(unsigned long long* src_keys,
                               uint32_t* dest_total_keys,
                               const long long* len, int slot_num,
                               int total_len, int* key2slots) {
  int cid = core_id();
  int ncores = core_num();
  if (cid >= ncores) {
    return;
  }
  int thread_id = ncores * cluster_id() + cid;
  int nthreads = ncores * cluster_num();
  __local__ long long local_len[slot_num + 1];
  GM2LM(len, local_len, (slot_num + 1) * sizeof(long long));

  __global_ptr__ unsigned long long* local_keys[slot_num];
  GM2LM(src_keys, local_keys,
        slot_num * sizeof(__global_ptr__ unsigned long long*));

  for (int i = thread_id; i < slot_num; i += nthreads) {
    // max core local memory = 8KB
    int slot_len = local_len[i + 1] - local_len[i];
    // int read_len = min(slot_len, 1024);
    int read_len = 100;
    int dest_offset = local_len[i];
    __local__ unsigned long long local_slot_keys[read_len];
    __local__ uint32_t local_slot_keys_uint32[read_len];
    __local__ int local_slot_uint32[read_len];

    for (int k = 0; k < slot_len; k += read_len) {
      int real_read_len = min(read_len, slot_len - k);
      GM2LM(local_keys[i] + k, local_slot_keys,
            real_read_len * sizeof(unsigned long long));
      for (int m = 0; m < real_read_len; m++) {
          local_slot_keys_uint32[m] = (uint32_t)local_slot_keys[m];
          local_slot_uint32[m] = i;
      }
      mfence();
      LM2GM(local_slot_uint32, key2slots + dest_offset + k, real_read_len * sizeof(int));
      LM2GM(local_slot_keys_uint32, dest_total_keys + dest_offset + k,
            real_read_len * sizeof(uint32_t));
    }
  }
}

void BoxWrapperKernel::CopyKeys(const paddle::platform::Place& place,
                            uint64_t** origin_keys, uint32_t* total_keys,
                            const int64_t* gpu_len, int slot_num,
                            int total_len, int* key2slots) {
  XPUStream stream = nullptr;
  auto dev_ctx = platform::DeviceContextPool::Instance().Get(place);
  stream = static_cast<platform::XPUDeviceContext*>(dev_ctx)
               ->x_context()
               ->xpu_stream;
  unsigned long long* o_keys =
      reinterpret_cast<unsigned long long*>(origin_keys);
  const long long* c_len = (const long long*)gpu_len;
  CopyKeysKernel<<<2, 64, stream>>>(o_keys, total_keys, c_len, slot_num, total_len, key2slots);
  xpu_wait(stream);
}

template <typename TEmbedxOp>
__global__ void PullCopy(const TEmbedxOp* op,
                          const float scale,
                          const boxps::FeaturePullOffset* info,
                          int* total_dims,
                          float* dst_vals,
                          float* total_values,
                          const uint32_t* restore_idx,
                          const int total_length,
                          const int hidden_size,
                          const int pull_float_num,
                          const int skip_offset,
                          const int cvm_offset) {
  int cid = core_id();
  int ncores = core_num();
  if (cid >= ncores) {
      return;
  }
  int thread_id = cluster_id() * ncores + cid;
  int nthreads = cluster_num() * ncores;

  const int buf_length = 30;
  int per_thread_len = roundup_div(total_length, nthreads);
  int per_thread_loop_count = roundup_div(per_thread_len, buf_length);
  int per_thread_per_loop_len = roundup_div(per_thread_len, per_thread_loop_count);

  __local__ float lm_total_values[buf_length * pull_float_num];
  __local__ float lm_dst_vals[buf_length * hidden_size];
  __local__ int lm_total_dims[buf_length];
  __local__ uint32_t lm_restore_idx[buf_length];
  __local__ boxps::FeaturePullOffset lm_info[1];
  __local__ TEmbedxOp lm_op[1];

  GM2LM(info, lm_info, sizeof(boxps::FeaturePullOffset));
  GM2LM(op, lm_op, sizeof(TEmbedxOp));
  for (int i = thread_id; i < per_thread_loop_count * nthreads; i += nthreads) {
    int gm_offset = i * per_thread_per_loop_len;
    if (gm_offset >= total_length) {
      return;
    }
    //if(restore_idx != nullptr) {
    //  GM2LM(restore_idx + gm_offset, lm_restore_idx, per_thread_per_loop_len * sizeof(uint32_t));
    //}
    //int pos = (restore_idx != nullptr) ? lm_restore_idx[gm_offset] : gm_offset;
    //GM2LM(total_values + pos * pull_float_num, lm_total_values, per_thread_per_loop_len * pull_float_num * sizeof(float));

    int len = min(per_thread_per_loop_len, total_length - gm_offset);
    GM2LM(total_values + gm_offset * pull_float_num, lm_total_values, len * pull_float_num * sizeof(float));
    GM2LM(total_dims + gm_offset, lm_total_dims, len * sizeof(int));

    for (int j = 0; j < len; j++) {
      for (int k = 0; k < cvm_offset; ++k) {
        lm_dst_vals[j * hidden_size + k] = lm_total_values[j * pull_float_num + lm_info[0].show + skip_offset + k];
      }
      // embedx
      int embedx_size = *((int *)&(lm_total_values[j * pull_float_num + lm_info[0].embedx_size]));
      lm_total_dims[j] = static_cast<int>(embedx_size > 0);
      for (int k = 0; k < embedx_size; ++k) {
        lm_op[0].copy(lm_dst_vals + j * hidden_size + cvm_offset + k,
                lm_total_values + j * pull_float_num + lm_info[0].embedx,
                k,
                scale);
      }
      int dim_size = hidden_size - cvm_offset;
      for (int k = embedx_size; k < dim_size; ++k) {
        lm_dst_vals[j * hidden_size + cvm_offset + k] = 0;
      }
    }
    mfence();
    LM2GM(lm_dst_vals, dst_vals + gm_offset * hidden_size, len * sizeof(float) * hidden_size);
    LM2GM(lm_total_dims, total_dims + gm_offset, len * sizeof(int));
  }
}

template <typename TEmbedxOp>
inline void FeaturePullCopy(const paddle::platform::Place& place,
                            const TEmbedxOp* op,
                            const float scale,
                            const boxps::FeaturePullOffset* info,
                            int* total_dims,
                            float** xpu_values,// const std::vector<float*>& values,
                            uint64_t* total_keys_xpu,
                            float* total_values_xpu,
                            const uint32_t* xpu_restore_idx,
                            const int64_t* slot_lens,
                            const int slot_num,
                            const int total_length,
                            const int hidden_size,
                            const int pull_float_num,
                            const int skip_offset,
                            const int cvm_offset){
  auto dev_ctx = platform::DeviceContextPool::Instance().Get(place);
  auto ctx_xpu = static_cast<platform::XPUDeviceContext*>(dev_ctx)->x_context();
  auto stream = ctx_xpu->xpu_stream;

  auto d_op_tmp = memory::Alloc(place, sizeof(TEmbedxOp));
  TEmbedxOp* d_op = reinterpret_cast<TEmbedxOp*>(d_op_tmp->ptr());
  memory::Copy(place,
               d_op,
               platform::CPUPlace(),
               op,
               sizeof(TEmbedxOp));

  TRACE_SCOPE_START("PullCopy", xpu_wait(stream));

  float* real_dst_vals;
  for (int i = 0; i < slot_num; i++) {
    if(xpu_values[i] != nullptr) {
      real_dst_vals = xpu_values[i];
      break;
    }
  }
  PullCopy<TEmbedxOp><<<8, 64, stream>>>(d_op,
                                scale,
                                info,
                                total_dims,
                                real_dst_vals,
                                total_values_xpu,
                                xpu_restore_idx,
                                total_length,
                                hidden_size,
                                pull_float_num,
                                skip_offset,
                                cvm_offset);
  xpu_wait(stream);
  TRACE_SCOPE_END("PullCopy", );

  TRACE_SCOPE_START("PullCopy's xpu::copy", xpu_wait(stream));
  xpu_wait(stream);
  TRACE_SCOPE_END("PullCopy's xpu::copy",);
}

void BoxWrapperKernel::CopyForPull(
    const paddle::platform::Place& place, uint64_t** gpu_keys,
    float** xpu_values, void* total_values_xpu,
    boxps::FeaturePullOffset* pull_offset, const int64_t* slot_lens,
    const int slot_num, const int* key2slot, const int hidden_size,
    const int expand_embed_dim, const int64_t total_length, int* total_dims,
    const int skip_offset, bool expand_only, const uint32_t* xpu_restore_idx) {
  CHECK(xpu_restore_idx == nullptr) << "Not Supported yet";
  uint64_t* total_keys_xpu = nullptr;
  const int cvm_offset = cvm_offset_ - skip_offset;
  if (pull_info_.is_quant) {
    EmbedxQuantOp op;
    FeaturePullCopy(place,
                    &op,
                    pull_embedx_scale_,
                    pull_offset,
                    total_dims,
                    xpu_values,
                    total_keys_xpu,
                    (float*)total_values_xpu,
                    xpu_restore_idx,
                    slot_lens,
                    slot_num,
                    (int)total_length,
                    hidden_size,
                    (int)pull_float_num_,
                    skip_offset,
                    cvm_offset);
  } else {
    EmbedxNormalOp op;
    FeaturePullCopy(place,
                    &op,
                    pull_embedx_scale_,
                    pull_offset,
                    total_dims,
                    xpu_values,
                    total_keys_xpu,
                    (float*)total_values_xpu,
                    xpu_restore_idx,
                    slot_lens,
                    slot_num,
                    total_length,
                    hidden_size,
                    pull_float_num_,
                    skip_offset,
                    cvm_offset);
  }
}

__global__ void PushCopy(float* src_vals,
    float* dest_vals,
    boxps::FeaturePushOffset* push_offset,
    const int push_float_num,
    const int total_length,
    const int hidden_size,
    const int batch_size,
    const int* total_dims,
    const int skip_offset,
    const int cvm_offset,
    const int* key2slot,
    const int* slots,
    const int slot_num) {
    int cid = core_id();
    int ncores = core_num();
    if (cid >= ncores) {
      return;
    }
    int thread_id = cluster_id() * ncores + cid;
    int nthreads = cluster_num() * ncores;

    const int buf_length = 40; // max 2048 float
    int per_thread_len = roundup_div(total_length, nthreads);
    int per_thread_loop_count = roundup_div(per_thread_len, buf_length);
    int per_thread_per_loop_len =
      roundup_div(per_thread_len, per_thread_loop_count);

    __local__ float lm_src_vals[buf_length * hidden_size];
    __local__ float lm_dest_vals[buf_length * push_float_num];
    __local__ int lm_total_dims[buf_length];
    __local__ int lm_key2slot[buf_length];
    boxps::FeaturePushOffset info;

    // shared memory max 256 KB per cluster
    const int max_slot_num = 1000;
    __shared__ int sm_slots[max_slot_num];
    int sm_slot_len = min(max_slot_num, slot_num);
    int lm_slot = -1;
    for (int i = cid; i < sm_slot_len; i += ncores) {
        mfence();
        GM2LM(slots + i, &lm_slot, sizeof(int));
        sm_slots[i] = lm_slot;
    }

    mfence();
    xpu_sync_all();

    GM2LM(push_offset, &info, sizeof(boxps::FeaturePushOffset));

    float scale = -1. * batch_size;
    for (int i = thread_id; i < per_thread_loop_count * nthreads; i += nthreads) {
      mfence();
      int gm_offset = i * per_thread_per_loop_len;
      if (gm_offset >= total_length)
        return;

      int count_per_loop =
          min(per_thread_per_loop_len, total_length - gm_offset);

      GM2LM(src_vals + gm_offset * hidden_size, lm_src_vals,
              count_per_loop * hidden_size * sizeof(float));
      //GM2LM(dest_vals + gm_offset * push_float_num, lm_dest_vals,
      //        count_per_loop * push_float_num * sizeof(float));
      GM2LM(total_dims + gm_offset, lm_total_dims,
              count_per_loop * sizeof(int));
      GM2LM(key2slot + gm_offset, lm_key2slot,
              count_per_loop * sizeof(int));

      for (int j = 0; j < count_per_loop; j++) {
          mfence();
          float* dest_val = &(lm_dest_vals[j * push_float_num]);
          if (lm_key2slot[j] < sm_slot_len) {
              lm_slot = sm_slots[lm_key2slot[j]];
              set_byfloat<int>(dest_val + info.slot, lm_slot);
          } else {
              mfence();
              GM2LM(slots + lm_key2slot[j], &lm_slot, sizeof(int));
              set_byfloat<int>(dest_val + info.slot, lm_slot);
          }

          float* optr = reinterpret_cast<float*>(&dest_val[info.show]);
          float* src_val = reinterpret_cast<float*>(lm_src_vals + j * hidden_size);

          for (int k = 0; k < skip_offset; ++k) {
              optr[k] = 1.0;
          }
          for (int k = 0; k < cvm_offset; ++k) {
              optr[k + skip_offset] = src_val[k];
          }
          for (int k = 0; k < info.embed_num; ++k) {
              dest_val[info.embed_g + k] *= scale;
          }

          if (lm_total_dims[j] & 0x01) {
              for (int k = 0; k < hidden_size - cvm_offset; ++k) {
                  dest_val[info.embedx_g + k] = src_val[cvm_offset + k] * scale;
              }
          } else {
              for (int k = 0; k < hidden_size - cvm_offset; ++k) {
                  dest_val[info.embedx_g + k] = 0;
              }
          }
      }

      mfence();
      LM2GM(&(lm_dest_vals[0]), dest_vals + gm_offset * push_float_num,
              count_per_loop * push_float_num * sizeof(float));
  }
}

void BoxWrapperKernel::CopyForPush(
    const paddle::platform::Place& place,
    float* gm_src_ptr,
    void* total_grad_values_xpu,
    boxps::FeaturePushOffset* push_offset,
    const int64_t total_length,
    const int* slots,
    const int64_t* slot_lens,
    const int slot_num,
    const int hidden_size,
    const int batch_size,
    const int* total_dims,
    const int skip_offset,
    const int* key2slot) {
  auto dev_ctx = platform::DeviceContextPool::Instance().Get(place);
  auto stream = static_cast<platform::XPUDeviceContext*>(dev_ctx)
               ->x_context()
               ->xpu_stream;

  const int cvm_offset = cvm_offset_ - skip_offset;

  const int c_total_length = static_cast<const int>(total_length);
  float* push_grad_values = reinterpret_cast<float*>(total_grad_values_xpu);

  PushCopy<<<8, 64, stream>>>(gm_src_ptr, push_grad_values, push_offset,
      push_float_num_, c_total_length, hidden_size, batch_size, total_dims,
      skip_offset, cvm_offset, key2slot, slots, slot_num);

  xpu_wait(stream);
}

void BoxWrapperKernel::GetFeatureInfo(boxps::FeaturePullOffset &pull_info,
    size_t feature_pull_size, boxps::FeaturePushOffset &push_info,
    size_t feature_push_size, int embedx_dim, int expand_embed_dim,
    float pull_embedx_scale) {
  pull_info_ = pull_info;
  feature_pull_size_ = feature_pull_size;
  push_info_ = push_info;
  feature_push_size_ = feature_push_size;

  pull_float_num_ = feature_pull_size_ / sizeof(float);
  push_float_num_ = feature_push_size_ / sizeof(float);
  // set cvm offset
  cvm_offset_ = pull_info_.embedx_size - pull_info_.show;

  embedx_dim_ = embedx_dim;
  expand_embed_dim_ = expand_embed_dim;
  pull_embedx_scale_ = pull_embedx_scale;
}

}  // end namespace framework
}  // end namespace paddle
#endif
